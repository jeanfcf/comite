# -*- coding: utf-8 -*-
"""MODELOS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J5H4_LNvhys4hbXG5zzvMtt7dfe0IMnW

# IMPORTS PYTHON
"""
# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV, cross_validate
from pandas import read_csv
import math
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from statsmodels.tsa.seasonal import seasonal_decompose
from keras.preprocessing.sequence import TimeseriesGenerator
from math import sqrt
from sklearn.model_selection import TimeSeriesSplit
from matplotlib import pyplot
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
import matplotlib
from pandas import datetime
from pandas import concat
from pandas import DataFrame
import seaborn as sns
import statsmodels as sm
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima_model import ARIMA
from sklearn.svm import SVR
from sklearn.utils import shuffle
from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_percentage_error,mean_absolute_error
import sklearn.metrics as metrics
from sklearn import preprocessing, svm
import warnings
from matplotlib.pylab import rcParams
from pmdarima.arima import auto_arima
import sys
# %load_ext rpy2.ipython
# %matplotlib inline
rcParams['figure.figsize'] = 15,6 
warnings.filterwarnings("ignore")

"""# PYTHON

### ARIMA
"""

dateparse = lambda dates: pd.datetime.strptime(dates,'%Y-%m')
df = pd.read_csv('./UK-dataset.csv',parse_dates=['ANO'], index_col='ANO',date_parser=dateparse)
df = df.drop(['CASOS'], 1)
df = df.drop(['MORTES'], 1)
df = df.drop(['GDP'], 1)
df = df.drop(['LIBRA'], 1)
df = df.drop(['DESEMPREGO'], 1)
df = df.drop(['BANKRATE'], 1)
df.head()

model_arima = auto_arima(df,m=12,trace=3,error_action='ignore',suppress_warnings=True)

arima_train = df.loc['1997-01-01':'2016-12-01']
arima_test = df.loc['2017-01-01':]

model_arima.fit(arima_train)

arima_forecast = model_arima.predict(n_periods=51)

arima_future = pd.DataFrame(arima_forecast,index=arima_test.index,columns=['#INFLACAO'])

figure, axis = plt.subplots(2, 3)

axis[0, 0].plot(df, '-', label="Real")
axis[0, 0].plot(arima_future, '-', color='red', label="Predito")
axis[0, 0].set_xlabel('TEMPO')
axis[0, 0].set_ylabel('TAXA DE INFLAÇÃO')
axis[0, 0].set_title('ARIMA')
axis[0, 0].legend()


arima_mse = mean_squared_error(arima_test,arima_future)
arima_rmse = np.sqrt(arima_mse)
arima_mae = metrics.mean_absolute_error(arima_test,arima_future)
arima_mape = metrics.mean_absolute_percentage_error(arima_test,arima_future)
arima_r2 = metrics.r2_score(arima_test,arima_future)

"""### ARIMAX

"""

arimax_exogenous = pd.read_csv('./UK-dataset.csv',parse_dates=['ANO'], index_col='ANO',date_parser=dateparse)
arimax_exogenous = arimax_exogenous.drop(['CASOS'], 1)
arimax_exogenous = arimax_exogenous.drop(['MORTES'], 1)
arimax_exogenous = arimax_exogenous.drop(['INFLACAO'], 1)
arimax_exogenous.head()

model_arimax = auto_arima(df,X=arimax_exogenous,m=12,trace=3,error_action='ignore',suppress_warnings=True)

arimax_train = df.loc['1997-01-01':'2016-12-01']
arimax_test = df.loc['2017-01-01':]

arimax_xtrain = arimax_exogenous.loc['1997-01-01':'2016-12-01']
arimax_xtest = arimax_exogenous.loc['2017-01-01':]

model_arimax.fit(arimax_train,X=arimax_xtrain)

arimax_xforecast = model_arimax.predict(n_periods=51,X=arimax_xtest)

arimax_xfuture = pd.DataFrame(arimax_xforecast,index=arimax_test.index,columns=['#INFLACAO'])


axis[0, 1].plot(df, '-', label="Real")
axis[0, 1].plot(arimax_xfuture, '-', color='red', label="Predito")
axis[0, 1].set_xlabel('TEMPO')
axis[0, 1].set_ylabel('TAXA DE INFLAÇÃO')
axis[0, 1].set_title('ARIMAX')
axis[0, 1].legend()

arimax_mse = mean_squared_error(arimax_test,arimax_xfuture)
arimax_rmse = np.sqrt(arimax_mse)
arimax_mae = metrics.mean_absolute_error(arimax_test,arimax_xfuture)
arimax_mape = metrics.mean_absolute_percentage_error(arimax_test,arimax_xfuture)
arimax_r2 = metrics.r2_score(arimax_test,arimax_xfuture)

"""### ARIMA + SVR

"""

residuo = pd.DataFrame(arima_test.INFLACAO - arima_future['#INFLACAO'] , columns=['residuos'])
residuo.head()

def mean_a_p_e(y_true, y_pred):
    sum = 0
    for i in range(len(y_true)):
        sum += math.fabs(y_pred[i] - y_true[i]) / y_true[i]
    return sum / len(y_true)


def parser(x):
    return datetime.strptime(x, '%Y-%m-%d %H:%M:%S')


series = residuo
X = series.values
X = X.astype('float32')

train_start = 0
train_end = 40

test_start = 41
test_end = 51

svr_train, svr_test = X[train_start:train_end], X[test_start - 1:test_end - 1]
svr_real = np.array(X[train_start:test_end]).reshape(-1, 1)

svr_train = np.array(svr_train).reshape(-1, 1)
svr_test = np.array(svr_test).reshape(-1, 1)

param_grid = { 'C':[0.1,1,100,1000],'kernel':['rbf','poly','sigmoid','linear'],'degree':[1,2,3,4,5,6],'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}
grid = GridSearchCV(SVR(),param_grid)

grid.fit(svr_train[0:train_end - train_start - 1], svr_train[1:train_end - train_start])
p = grid.predict(svr_real)

# plt.figure(figsize=(12, 6))
# plt.plot(X, '-', color='r')
# plt.legend(loc='upper right')
# plt.xlabel("tempo")
# plt.ylabel("residuos")
# plt.plot(p)
# plt.show()

preds_svr = arima_future['#INFLACAO']+p

arimasvr_mse = mean_squared_error(arima_test.INFLACAO,preds_svr)
arimasvr_rmse = math.sqrt(mean_squared_error(arima_test.INFLACAO,preds_svr))
arimasvr_mae = mean_absolute_error(arima_test.INFLACAO,preds_svr)
arimasvr_mape = mean_a_p_e(arima_test.INFLACAO,preds_svr)
# print('SVR arima_test MAE:%.3f MSE: %.3f RMSE:%.3f MAPE:%.3f' % (arimasvr_mae, arimasvr_mse, arimasvr_rmse, arimasvr_mape))
# axis[0, 1].plot(arima_test.INFLACAO)
# axis[0, 1].plot(preds_svr)

axis[1, 0].plot(df, '-', label="Real")
axis[1, 0].plot(preds_svr, '-', color='red', label="Predito")
axis[1, 0].set_xlabel('TEMPO')
axis[1, 0].set_ylabel('TAXA DE INFLAÇÃO')
axis[1, 0].set_title('ARIMA-SVR')
axis[1, 0].legend()

arimasvr2_mse = mean_squared_error(arima_test.INFLACAO,preds_svr)
arimasvr2_rmse = np.sqrt(arimasvr2_mse)
arimasvr2_mae = metrics.mean_absolute_error(arima_test.INFLACAO,preds_svr)
arimasvr2_mape = metrics.mean_absolute_percentage_error(arima_test.INFLACAO,preds_svr)
arimasvr2_r2 = metrics.r2_score(arima_test.INFLACAO,preds_svr)
# print("MAE", arimasvr2_mae)
# print("MSE:", arimasvr2_mse)
# print("RMSE:", arimasvr2_rmse)
# print("MAPE:", arimasvr2_mape)
# print("R-Squared:", arimasvr2_r2)

"""### ARIMA + MLP"""

residuo.head()

X = np.array(arima_test)
y = np.array(residuo)

X = X.reshape(-1,1)
y = y.reshape(-1,1)

X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=False)

tscv = TimeSeriesSplit(n_splits=3)

mlpr = MLPRegressor(max_iter=7000)

param_list = {"hidden_layer_sizes": [(1,),(50,)], "activation": ["identity", "logistic", "tanh", "relu"], "solver": ["lbfgs", "sgd", "adam"], "alpha": [0.00005,0.0005]}
gridCV = GridSearchCV(estimator=mlpr, param_grid=param_list)

gridCV.fit(X_train, y_train)

gridCV.best_params_

arimamlp_y_true, arimamlp_y_pred = y_validate , gridCV.predict(X_validate)

arimamlp_y_pred = arimamlp_y_pred.reshape(-1,1)

arimamlp_mse = mean_squared_error(arimamlp_y_true,arimamlp_y_pred)
arimamlp_rmse = np.sqrt(arimamlp_mse)
arimamlp_mae = metrics.mean_absolute_error(arimamlp_y_true,arimamlp_y_pred)
arimamlp_mape = mean_absolute_percentage_error(arimamlp_y_true,arimamlp_y_pred)
arimamlp_r2 = metrics.r2_score(arimamlp_y_true,arimamlp_y_pred)
# print("MAE", arimamlp_mae)
# print("MSE:", arimamlp_mse)
# print("RMSE:", arimamlp_rmse)
# print("MAPE:", arimamlp_mape)
# print("R-Squared:", arimamlp_r2)
# plt.plot(arimamlp_y_pred, '-', label="predicted")
# plt.plot(arimamlp_y_true, '--', color='red', label="Real")
# plt.legend(loc='upper right')
# plt.xlabel("tempo")
# plt.ylabel("residuos")
# plt.figure(figsize=(12, 6))
# plt.show()

arimamlp_predicao = arima_future['#INFLACAO']

arimamlp_tamanho = math.floor(len(arimamlp_predicao)*0.7)

arimamlp_predicao = arimamlp_predicao[arimamlp_tamanho:]

arimamlp_real = arima_test.INFLACAO[arimamlp_tamanho:]

arimamlp_predicao = np.array(arimamlp_predicao)
arimamlp_real = np.array(arimamlp_real)

arimamlp_predicao = arimamlp_predicao.reshape(-1,1)
arimamlp_real = arimamlp_real.reshape(-1,1)

preds_mlp = arimamlp_predicao + arimamlp_y_pred

arimamlp2_mse = mean_squared_error(arimamlp_real,preds_mlp)
arimamlp2_rmse = np.sqrt(arimamlp2_mse)
arimamlp2_mae = metrics.mean_absolute_error(arimamlp_real,preds_mlp)
arimamlp2_mape = mean_absolute_percentage_error(arimamlp_real,preds_mlp)
arimamlp2_r2 = metrics.r2_score(arimamlp_real,preds_mlp)
# print("MAE", arimamlp2_mae)
# print("MSE:", arimamlp2_mse)
# print("RMSE:", arimamlp2_rmse)
# print("MAPE:", arimamlp2_mape)
# print("R-Squared:", arimamlp2_r2)
# plt.plot(preds_mlp, '-', label="predicted")
# plt.plot(arimamlp_real, '--', color='red', label="Real")
# plt.legend(loc='upper right')
# plt.xlabel("tempo")
# plt.ylabel("residuos")
# plt.figure(figsize=(12, 6))
# plt.show()



t = pd.DataFrame(arima_future['#INFLACAO'])

f = pd.DataFrame(preds_mlp,columns=['preds'])

t['#INFLACAO'][arimamlp_tamanho:] = f.preds

# pd.concat([df,t],axis=1).plot()

axis[1, 1].plot(df, '-', label="Real")
axis[1, 1].plot(t, '-', color='red', label="Predito")
axis[1, 1].set_xlabel('TEMPO')
axis[1, 1].set_ylabel('TAXA DE INFLAÇÃO')
axis[1, 1].set_title('ARIMA-MLP')
axis[1, 1].legend()

"""### LSTM

"""

# converte um array de valores em uma matriz
def create_dataset(dataset, look_back=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-look_back-1):
		a = dataset[i:(i+look_back), 0]
		dataX.append(a)
		dataY.append(dataset[i + look_back, 0])
	return np.array(dataX), np.array(dataY)

# fixa o random para dar o mesmo resultado
np.random.seed(7)

dataframe = df
dataset = dataframe.values
dataset = dataset.astype('float32')

# normaliza os dados
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataset)

# treino e teste
train_size = int(len(dataset) * 0.80)
test_size = len(dataset) - train_size
lstm_train, lstm_test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]

# reshape em X=t e Y=t+1
look_back = 1
lstm_trainX, lstm_trainY = create_dataset(lstm_train, look_back)
lstm_testX, lstm_testY = create_dataset(lstm_test, look_back)

# reshape entradas para [samples, time steps, features]
lstm_trainX = np.reshape(lstm_trainX, (lstm_trainX.shape[0], 1, lstm_trainX.shape[1]))
lstm_testX = np.reshape(lstm_testX, (lstm_testX.shape[0], 1, lstm_testX.shape[1]))

# cria e fita o LSTM
model_lstm = Sequential()
model_lstm.add(LSTM(4, input_shape=(1, look_back)))
model_lstm.add(Dense(1))
model_lstm.compile(loss='mean_squared_error', optimizer='adam')
model_lstm.fit(lstm_trainX, lstm_trainY, epochs=100, batch_size=1, verbose=2)

lstm_trainPredict = model_lstm.predict(lstm_trainX)
lstm_testPredict = model_lstm.predict(lstm_testX)

# transforma os dados normalizados de volta
lstm_trainPredict = scaler.inverse_transform(lstm_trainPredict)
lstm_trainY = scaler.inverse_transform([lstm_trainY])
lstm_testPredict = scaler.inverse_transform(lstm_testPredict)
lstm_testY = scaler.inverse_transform([lstm_testY])

# calculando RMSE
trainScore = math.sqrt(mean_squared_error(lstm_trainY[0], lstm_trainPredict[:,0]))
# print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(lstm_testY[0], lstm_testPredict[:,0]))
# print('Test Score: %.2f RMSE' % (testScore))

LSTM_mse = mean_squared_error(lstm_testY[0], lstm_testPredict[:,0])
LSTM_rmse = np.sqrt(LSTM_mse)
LSTM_mae = metrics.mean_absolute_error(lstm_testY[0], lstm_testPredict[:,0])
LSTM_mape = metrics.mean_absolute_percentage_error(lstm_testY[0], lstm_testPredict[:,0])
LSTM_r2 = metrics.r2_score(lstm_testY[0], lstm_testPredict[:,0])
# print("MAE", LSTM_mae)
# print("MSE:", LSTM_mse)
# print("RMSE:", LSTM_rmse)
# print("MAPE:", LSTM_mape)
# print("R-Squared:", LSTM_r2)

# desloca as predições de treino para plotar no grafico
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(lstm_trainPredict)+look_back, :] = lstm_trainPredict

# desloca as predições de teste para plotar no grafico
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(lstm_trainPredict)+(look_back*2)+1:len(dataset)-1, :] = lstm_testPredict

# plotando dataser e predicoes
# plt.plot(scaler.inverse_transform(dataset))
# plt.plot(trainPredictPlot)
# plt.plot(testPredictPlot)
# plt.show()

axis[1, 2].plot(scaler.inverse_transform(dataset), '-', label="Real")
axis[1, 2].plot(trainPredictPlot, '-', color='red', label="TRAIN_Predito")
axis[1, 2].plot(testPredictPlot, '-', color='green', label="TEST_Predito")
axis[1, 2].set_xlabel('TEMPO')
axis[1, 2].set_ylabel('TAXA DE INFLAÇÃO')
axis[1, 2].set_title('LSTM')
axis[1, 2].legend()

"""### PCA

"""

dateparse = lambda dates: pd.datetime.strptime(dates,'%Y-%m')
df_pca = pd.read_csv('./UK-dataset.csv',parse_dates=['ANO'], index_col='ANO',date_parser=dateparse,engine='python')
df_pca = df_pca.drop(['CASOS'], 1)
df_pca = df_pca.drop(['MORTES'], 1)
df_pca = df_pca.drop(['INFLACAO'], 1)
df_pca.head()

pca = PCA()

X_pca = df_pca

# normaliza os dados
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(X_pca)

pca.fit(dataset)

pca.explained_variance_ratio_

feature_importances_pca = pd.DataFrame(pca.explained_variance_ratio_.reshape(-1,len(X_pca.columns)),
                                   index = ['importance'],
                                   columns=X_pca.columns)
feature_importances_pca



"""### RANDOM FOREST"""

dateparse = lambda dates: pd.datetime.strptime(dates,'%Y-%m')
df_rf = pd.read_csv('./UK-dataset.csv',parse_dates=['ANO'], index_col='ANO',date_parser=dateparse,engine='python')
df_rf = df_rf.drop(['CASOS'], 1)
df_rf = df_rf.drop(['MORTES'], 1)
df_rf.head()

# Importing the datasets

datasets = df_rf
datasets = datasets.astype('float32')

X_rf = df_rf.iloc[:, [1,2,3,4]]
Y_rf = df_rf.iloc[:, 0]

# Splitting the dataset into the Training set and Test set

X_Train, X_Test, Y_Train, Y_Test = train_test_split(X_rf, Y_rf, test_size = 0.3, random_state = 0)

# Feature Scaling

sc_X = StandardScaler()

X_Train = sc_X.fit_transform(X_Train)
X_Test = sc_X.transform(X_Test)

# converte um array de valores em uma matriz
def format_dataset(dataset):
  dataX = []
  for i in range (len(dataset)):
    a = dataset[i]
    for n in range (len(a)):
      a[n] = format(a[n],'.1f')
      #print (a[n])
    #print (a)
    dataX.append(a)
  return np.array(dataX)

def format_test(dataset):
  dataX = []
  for i in range (len(dataset)):
    a = format(dataset[i],'.1f')
    dataX.append(a)
  return np.array(dataX)

X_Train = format_dataset(X_Train)
Y_Train = format_test(Y_Train)

model_rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=10,
                       n_jobs=None, oob_score=False, random_state=1,
                       verbose=0, warm_start=False)
model_rf.fit(X_Train, Y_Train)
feature_importances_rf = pd.DataFrame(model_rf.feature_importances_.reshape(-1,len(X_rf.columns)),columns=X_rf.columns,index=['importance'])
feature_importances_rf

def format_string(dataset):
  dataX = []
  for i in range (len(dataset)):
    a = str(dataset[i])
    dataX.append(a)
  return np.array(dataX)

def format_float(dataset):
  dataX = []
  for i in range (len(dataset)):
    a = float(dataset[i])
    dataX.append(a)
  return np.array(dataX)

"""### Comitê"""

colunas_pca = []
for x in range(int(len(X_rf.columns)/2)):
  colunas_pca.append(x)

feature_importances_pca.iloc[:,colunas_pca]

colunas_rf = [0,0]
for i in range(len(X_pca.columns)-1,-1,-1):
  va = feature_importances_rf[X_pca.columns[i]][0]
  flag = False
  if(va > colunas_rf[0]):     
      colunas_rf[0] = i
      flag = True
  else:
    if(flag == False):
      if(va > feature_importances_rf[X_pca.columns[colunas_rf[1]]][0]):
        colunas_rf[1] = i

feature_importances_rf.iloc[:,colunas_rf]

data =np.array([['ARIMA',arima_mse, arima_rmse, arima_mae,arima_mape,arima_r2],
['ARIMAX',arimax_mse, arimax_rmse, arimax_mae,arimax_mape,arimax_r2],
['ARIMA-MLP',arimamlp2_mse, arimamlp2_rmse, arimamlp2_mae,arimamlp2_mape,arimamlp2_r2],
['ARIMA-SVR',arimasvr2_mse, arimasvr2_rmse, arimasvr2_mae,arimasvr2_mape,arimasvr2_r2],
['LSTM',LSTM_mse, LSTM_rmse, LSTM_mae,LSTM_mape,LSTM_r2]])

df_result = pd.DataFrame(data,columns=['MODELO','MSE', 'RMSE', 'MAE','MAPE','R2'])
df_result.to_excel('./results.xlsx')
plt.show()
